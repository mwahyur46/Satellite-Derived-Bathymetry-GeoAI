{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73c6a185",
   "metadata": {},
   "source": [
    "# Pre-processing \n",
    "Tahap ini digunakan untuk mempersiapkan dataset yang digunakan dalam pembuatan model GeoAI untuk Satellite-Derived Bathymetry, langkah-langkahnya antara lain adalah: <br>\n",
    "1. Koreksi sunglint (deglint) untuk meminimalkan efek bayangan atau pantulan cahaya matahari di perairan yang terdapat pada citra remote sensing (dalam hal ini yang digunakan adalah Sentinel-2) berdasarkan metode Hedley e al. (2005).\n",
    "2. Menghitung Modified Normalized Difference Water Index (MNDWI) untuk masking daratan dan optically shallow water. Sehingga output yang dihasilkan adalah citra yang siap digunakan untuk training model.\n",
    "3. Ekstraksi nilai piksel dengan titik-titik berisi nilai kedalaman perairan dangkal yang didapatkan dari sounding menggunakan echosounder.\n",
    "4. Split train/test dataset untuk digunakan dalam tahapan model development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1b2f41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio as rio\n",
    "from tqdm import tqdm\n",
    "\n",
    "from preprocessing.preprocessing import sunglint_correction, calculate_mndwi, extract_raster_value\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1909d5d",
   "metadata": {},
   "source": [
    "## 1. Deglint and shallow water masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea1138e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 images.\n"
     ]
    }
   ],
   "source": [
    "# Configurations\n",
    "input_path = r'data'\n",
    "output_path = r'data\\corrected' # Output folder name\n",
    "plot = r'data\\corrected\\plot' # For QC plots\n",
    "\n",
    "# Parameters\n",
    "mndwi_threshold = 0.0 # Threshold for SWIR-based water detection\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "os.makedirs(plot, exist_ok=True)\n",
    "\n",
    "tif_files = glob.glob(os.path.join(input_path, \"*.tif\"))\n",
    "print(f\"Found {len(tif_files)} images.\")\n",
    "\n",
    "# Sentinel-2 Band Names for Metadata\n",
    "s2_band = [\n",
    "    'B1', 'B2', 'B3', 'B4', 'B5', 'B6',\n",
    "    'B7', 'B8', 'B8A', 'B9', 'B11', 'B12'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbad77ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hedley + MNDWI Processing: 100%|██████████| 8/8 [00:08<00:00,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. Check 'Plots' folder for Hedley regression graphs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Batch Processing for Sunglint Correction + MNDWI Masking\n",
    "for filepath in tqdm(tif_files, desc=\"Hedley + MNDWI Processing\"):\n",
    "    filename = os.path.basename(filepath)\n",
    "    \n",
    "    # Define output path using the FOLDER variable (prevents recursive error)\n",
    "    current_output_path = os.path.join(output_path, f\"corrected_{filename}\")\n",
    "\n",
    "    try:\n",
    "        with rasterio.open(filepath) as src:\n",
    "            # 1. Read Data\n",
    "            data = src.read().astype('float32')\n",
    "            profile = src.profile.copy()\n",
    "            \n",
    "            if src.count < 12:\n",
    "                print(f\"[SKIP] {filename}: Not enough bands ({src.count}). Need 12.\")\n",
    "                continue\n",
    "\n",
    "            # 2. Identify Bands Indexes\n",
    "            idx_blue, idx_green, idx_red = 1, 2, 3\n",
    "            idx_nir = 7   # Band 8 (Used for Hedley Glint Calc)\n",
    "            idx_swir = 10 # Band 11 (Used for MNDWI Masking)\n",
    "            \n",
    "            raw_blue = data[idx_blue]\n",
    "            raw_green = data[idx_green]\n",
    "            raw_red = data[idx_red]\n",
    "            raw_nir = data[idx_nir]\n",
    "            raw_swir = data[idx_swir]\n",
    "\n",
    "            # 3. Hedley Sunglint Corrections (RGB Only)\n",
    "            # This calculates the slope automatically and removes glint\n",
    "            corrected_bands = sunglint_correction(\n",
    "                visible_bands=[raw_blue, raw_green, raw_red],\n",
    "                nir_band=raw_nir,\n",
    "                output_dir=plot,\n",
    "                image_id=filename,\n",
    "                plot_graphs=True # Check the plots to see the regression slope!\n",
    "            )\n",
    "            \n",
    "            clean_blue = corrected_bands[0]\n",
    "            clean_green = corrected_bands[1]\n",
    "            clean_red = corrected_bands[2]\n",
    "            \n",
    "            # 4. MNDWI Water Masking\n",
    "            # Use Corrected Green + Raw SWIR\n",
    "            # SWIR is much better at ignoring glint than NIR\n",
    "            mndwi = calculate_mndwi(clean_green, raw_swir)\n",
    "            water_mask = mndwi > mndwi_threshold\n",
    "\n",
    "            # 5. Reconstruct Final Stack\n",
    "            final_stack = data.copy()\n",
    "            \n",
    "            # Overwrite RGB with Clean Versions\n",
    "            final_stack[idx_blue] = clean_blue\n",
    "            final_stack[idx_green] = clean_green\n",
    "            final_stack[idx_red] = clean_red\n",
    "            \n",
    "            # Apply Mask to ALL 12 bands\n",
    "            for b in range(12):\n",
    "                final_stack[b] = np.where(water_mask, final_stack[b], 0)\n",
    "\n",
    "            # 6. Save\n",
    "            profile.update(\n",
    "                dtype='float32',\n",
    "                nodata=0,\n",
    "                count=12\n",
    "            )\n",
    "\n",
    "            with rasterio.open(current_output_path, 'w', **profile) as dst:\n",
    "                dst.descriptions = tuple(s2_band)\n",
    "                dst.write(final_stack)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] {filename}: {e}\")\n",
    "\n",
    "print(\"Processing complete. Check 'Plots' folder for Hedley regression graphs.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3171e3b",
   "metadata": {},
   "source": [
    "# 2. Train/Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef40d221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "104de9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sounding data...\n",
      "           x        y   z1                           geometry\n",
      "0  113.24407 -7.67952 -1.4  POINT Z (113.24407 -7.67952 -1.4)\n",
      "1  113.24416 -7.67946 -1.4  POINT Z (113.24416 -7.67946 -1.4)\n",
      "2  113.24416 -7.67946 -1.6  POINT Z (113.24416 -7.67946 -1.6)\n",
      "3  113.24422 -7.67935 -1.6  POINT Z (113.24422 -7.67935 -1.6)\n",
      "4  113.24424 -7.67926 -1.4  POINT Z (113.24424 -7.67926 -1.4)\n",
      "Total Sounding Points found in file: 1055\n"
     ]
    }
   ],
   "source": [
    "# --- 1. CONFIGURATION ---\n",
    "sounding_path = r'data\\sounding\\sounding.shp' # Sample points from field survey with echosounder\n",
    "raster_path = r'data\\corrected\\corrected_s2_giliketapang_2018-05-31.tif'\n",
    "output_path = r'train-test dataset'\n",
    "depth_column = 'z1' # Column name in the shapefile containing depth data\n",
    "\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# --- 2. LOAD DATA ---\n",
    "print(\"Loading sounding data...\")\n",
    "# Read the shapefile\n",
    "sample_points = gpd.read_file(sounding_path)\n",
    "\n",
    "print(sample_points.head())\n",
    "print(f\"Total Sounding Points found in file: {len(sample_points)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d799ba66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting pixel values from: corrected_s2_giliketapang_2018-05-31.tif\n",
      "Processing: corrected_s2_giliketapang_2018-05-31.tif\n",
      "  - Reprojecting points from EPSG:4326 to EPSG:32749...\n",
      "  - Extracting pixel values for 1055 points...\n",
      "Cleaning data (removing NaNs and NoData)...\n",
      "Original Samples: 1055\n",
      "Valid Samples (After cleaning): 1054\n",
      "Splitting dataset...\n"
     ]
    }
   ],
   "source": [
    "# --- 3. EXECUTE EXTRACTION ---\n",
    "print(f\"Extracting pixel values from: {os.path.basename(raster_path)}\")\n",
    "X_features, y_labels = extract_raster_value(sample_points, raster_path, depth_column)\n",
    "\n",
    "# --- 4. DATA CLEANING (Handle \"Holes\") ---\n",
    "print(\"Cleaning data (removing NaNs and NoData)...\")\n",
    "\n",
    "# A. Remove NaNs (standard invalid data)\n",
    "no_nan_pixels = ~np.isnan(X_features).any(axis=1)\n",
    "no_nan_labels = ~np.isnan(y_labels)\n",
    "\n",
    "# B. Remove Zeros (common GEE mask/hole value)\n",
    "# If a pixel is 0.0 in ALL bands, it is likely a masked \"hole\"\n",
    "not_pure_black = np.all(X_features != 0, axis=1)\n",
    "\n",
    "# Combine filters\n",
    "valid_mask = no_nan_pixels & no_nan_labels & not_pure_black\n",
    "\n",
    "X_clean = X_features[valid_mask]\n",
    "y_clean = y_labels[valid_mask]\n",
    "\n",
    "print(f\"Original Samples: {len(X_features)}\")\n",
    "print(f\"Valid Samples (After cleaning): {len(X_clean)}\")\n",
    "\n",
    "# --- 5. SPLIT DATASET ---\n",
    "# 70% Training, 30% Testing (can be adjusted as needed)\n",
    "print(\"Splitting dataset...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_clean, y_clean, test_size=0.3, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa6c4bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving .npy files...\n",
      "------------------------------\n",
      "Success! Data saved in 'train-test dataset'\n",
      "Train Shape: X=(737, 12), y=(737,)\n",
      "Test Shape:  X=(317, 12), y=(317,)\n"
     ]
    }
   ],
   "source": [
    "# --- 6. SAVE RESULTS ---\n",
    "print(\"Saving .npy files...\")\n",
    "np.save(os.path.join(output_path, 'X_train.npy'), X_train)\n",
    "np.save(os.path.join(output_path, 'X_test.npy'), X_test)\n",
    "np.save(os.path.join(output_path, 'y_train.npy'), y_train)\n",
    "np.save(os.path.join(output_path, 'y_test.npy'), y_test)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"Success! Data saved in '{output_path}'\")\n",
    "print(f\"Train Shape: X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"Test Shape:  X={X_test.shape}, y={y_test.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geoai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
